---
description: Call this rule to execute complex commands that generate an experiment result to be saved.
globs: 
alwaysApply: false
---

## TLDR
Execute potentially long-running commands (experiments, data processing, model training), analyze logs and results, document outcomes, and save observations to memory.

## Instructions

1.  **Planning & Preparation**: Plan the command execution.
    *   `<think>`
        *   Identify the exact command(s) to execute based on the task context.
        *   Determine necessary arguments, input/output paths, and configurations.
        *   Estimate potential execution time.
        *   Consider if adjustments are needed for testing purposes (e.g., using a reduced dataset, fewer iterations) versus a full run.
        *   Consult relevant project files, documentation, or `techContext.md` if needed to confirm command details.
        *   Define expected outputs (files, database entries, log patterns).
        *   Outline the analysis steps needed for the results.
        *</think>`
    *   Summarize the plan and the command(s) to be executed.
    *   **(Experience-Execution: 1 - Planning completed)**

2.  **Command Execution**: Run the planned command(s).
    *   Use the `mcp_MyMCP_execute_command` MCP tool.
    *   Provide the `command` string.
    *   Set an appropriate `timeout` (in seconds) if the command might be long-running. Note: the command may continue in the background if the timeout is reached.
    *   The tool returns the `pid` and initial status/output.
    *   **(Experience-Execution: 2 - Command execution initiated)**

3.  **Result Analysis**: **Critically analyze** logs **AND generated files**.
    *   Examine the captured standard output and error streams for obvious errors or success indicators.
    *   **MANDATORY: Check for expected generated files** (defined in Step 1). Verify their existence.
    *   **MANDATORY: Critically analyze the CONTENT of key generated files** (e.g., result data, logs, configuration outputs). Use `read_file` if necessary.
        *   **Do NOT just check for existence or basic format.**
        *   **Look for:**
            *   Values within expected ranges.
            *   Absence of error messages or stack traces within log files.
            *   Correct structure and data types in output files (e.g., CSV columns, JSON structure).
            *   Any unusual patterns, outliers, or nonsensical results.
            *   Consistency between different output files.
        *   Verify that their format and content **truly conform** to expectations (defined in Step 1).
    *   `<think>`
        *   Compare the actual results (**including detailed file content analysis**) against the expected outcomes defined in the planning phase.
        *   Identify any discrepancies, errors, **anomalies**, unexpected results, or formatting issues found in the console output OR the generated files.
        *   Determine if the execution was successful based on the **complete and CRITICAL analysis** (console and files).
        *   **Explicitly note any strange findings**, even if not technically an error according to initial expectations.
        *</think>`
    *   Summarize the **detailed critical analysis** findings, explicitly mentioning the state and **content assessment** of generated files and any anomalies found.
    *   **(Experience-Execution: 3 - Critical Analysis completed)**

4.  **Documentation & Memory**: Document results and save observations.
    *   **4.1 Documentation (MANDATORY)**:
        *   **Create or update a dedicated documentation file** for this execution (e.g., create `results/[timestamp_or_name]/README.md`).
        *   **Content Requirements:** The file must include:
            *   The exact command executed.
            *   Key parameters used.
            *   A summary of the results based on the detailed analysis in Step 3 (including file status).
            *   Conclusions from the analysis (success/failure, key findings).
            *   **Direct links** to important generated files (logs, results).
        *   **Ensure discoverability:** Link or reference this documentation file from a central project location (e.g., the main `README.md`, an experiments index).
        *   **(Experience-Execution: 4.1 - Documentation updated)**
    *   **4.2 Memory Storage**:
        *   Use `mcp_Memory_create_entities` and/or `mcp_Memory_add_observations`.
        *   Create an entity representing the experiment (e.g., `Experiment:[Name]`).
        *   Add observations detailing:
            *   Command executed
            *   Key parameters
            *   Summary of results (success/failure, key metrics)
            *   Location of detailed documentation/results files
            *   Any significant findings or learnings
        *   **(Experience-Execution: 4.2 - Memory updated)**

5.  **MCP Terminal Cleanup**: Ensure the executed command's resources are released.
    *   `<think>`
        *   Identify the `pid` of the command executed in Step 2. This might have been returned by `mcp_execute_command` or needs to be found using `mcp_get_terminal_status` based on the command string or recent activity.
        *   Determine if the process with that `pid` still needs cleanup (e.g., if it finished but wasn't automatically cleaned by the MCP system).
        *</think>`
    *   Call `mcp_get_terminal_status` if the `pid` is unknown or needs confirmation.
    *   Call `mcp_stop_terminal_command` with the correct `pid` to terminate and clean up the process if necessary.
    *   **(Experience-Execution: 5 - MCP cleanup performed)**

6.  **Calling the next rule**: Decide the next step based on analysis.
    *   `<think>`
        *   Review the critical analysis summary from Step 3.
        *   Did the analysis explicitly note any anomalies or strange findings (even if not strictly errors)?
        *   Did the analysis conclude the execution failed or results were non-conformant?
        *</think>`
    *   **Decision Logic:**
        *   IF analysis (Step 3) detected problems, errors, or non-conformity → call `fix`, providing context about the failed execution and analysis findings.
        *   ELSE IF analysis (Step 3) explicitly noted **anomalies or strange findings** (even if the execution technically succeeded) → call `request-analysis`, instructing it to create a new task in `tasks.md` to investigate these specific anomalies.
        *   ELSE (execution successful and no anomalies noted) → call `context-update`.
    *   **(Experience-Execution: 6 - Calling next rule)**

## Specifics
-   This rule is intended for executing tasks that involve running commands/scripts and analyzing their output, distinct from unit testing (`test-execution`).
-   Ensure thorough planning in Step 1 to avoid command errors.
-   Result analysis (Step 3) is crucial for determining success or failure.
-   Documentation and Memory storage (Step 4) are mandatory for preserving experiment context and results.
-   Use `<think>` blocks for planning and analysis steps.
-   Adapt command execution (e.g., reduced scope) during planning if the goal is only to test functionality rather than perform a full run.

## Next Rules
-   `context-update` - If execution was successful and NO anomalies were noted.
-   `fix` - If execution failed or results were non-conformant.
-   `request-analysis` - If execution succeeded BUT anomalies/strange results were noted during critical analysis.

## Example

# Experience-Execution: 1 - Planning & Preparation
<think>
The task is to train the primary classification model.
Command: `python src/training/train_model.py --config configs/primary_classifier.yaml --output results/primary_run_$(date +%Y%m%d)`
Arguments: Config file specifies dataset, hyperparameters. Output dir for model and logs.
Time: Expected ~2 hours for full run. For testing, could use `--config configs/test_classifier.yaml` (~5 mins). Current task requires full run.
Expected outputs: `results/primary_run_*/model.pt`, `results/primary_run_*/training.log`, log file should contain "Training finished successfully".
Analysis: Check log for success message, verify model file exists.
</think>
Plan: Execute full training run using primary config. Verify log message and model file.
**(Experience-Execution: 1 - Planning completed)**

# Experience-Execution: 2 - Command Execution
Executing the training script using MCP.
[...calling tool `mcp_MyMCP_execute_command` with command="python src/training/train_model.py --config configs/primary_classifier.yaml --output results/primary_run_$(date +%Y%m%d)", timeout=7200...]
**(Experience-Execution: 2 - Command execution initiated)**
[...Use `mcp_MyMCP_get_terminal_status` and `mcp_MyMCP_get_terminal_output` to monitor and retrieve results...]

# Experience-Execution: 3 - Result Analysis
[...Checking log file `results/primary_run_YYYYMMDD/training.log`...]
[...Checking for `results/primary_run_YYYYMMDD/model.pt`...]
<think>
Log file contains "Training finished successfully". Model file `model.pt` exists. Execution appears successful and conforms to expectations.
</think>
Analysis: Training completed successfully based on log message and presence of model file.
**(Experience-Execution: 3 - Analysis completed)**

# Experience-Execution: 4 - Documentation & Memory

## 4.1 Documentation
[...Creating `results/primary_run_YYYYMMDD/README.md`...]
[...Adding execution details, parameters, link to log, summary of success to README...]
**(Experience-Execution: 4.1 - Documentation updated)**

## 4.2 Memory Storage
[...calling tool 'mcp_Memory_create_entities' with entity: Experiment:PrimaryClassifierTrain-YYYYMMDD...]
[...calling tool 'mcp_Memory_add_observations' with details for Experiment:PrimaryClassifierTrain-YYYYMMDD...]
**(Experience-Execution: 4.2 - Memory updated)**

# Experience-Execution: 5 - MCP Terminal Cleanup
<think>
The command `python src/training/train_model.py ...` was executed. Assuming its pid was 12345.
</think>
Checking status and cleaning up PID 12345 if necessary.
[...calling tool 'mcp_stop_terminal_command' with pid=12345...]
**(Experience-Execution: 5 - MCP cleanup performed)**

# Experience-Execution: 6 - Calling the next rule
Execution was successful. Calling `context-update`.
**(Experience-Execution: 6 - Calling next rule)**
[...calling tool 'fetch_rules' with rule_names=["context-update"]...]

### Important note on command execution

When executing commands using the `mcp_MyMCP_execute_command` tool, make sure that the command is correctly formatted.
