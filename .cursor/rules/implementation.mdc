---
description: Call this rule to implement the project's priority tasks.
globs: 
alwaysApply: false
---
## TLDR
Methodically implement tasks from a priority section, retrieve high-level context when needed, and document the process.

## Instructions

1. **Task analysis**: Identify the priority section
   - Read `.cursor/memory-bank/workflow/tasks.md`
   - Priority: first "In Progress" section, then first "ToDo" section
   - If all tasks completed, move to the next rule

2. **Active context update**: Document current work context
   - Read `.cursor/memory-bank/context/activeContext.md` to preserve useful information
   - Update the "Current implementation context" section with:
     - Tasks to perform and their logic
     - Relevant information researched online for the task
     - Particular attention points and dependencies
     - Technical decisions to make
   - Keep only information useful for the current task
   - Ensure this file contains all necessary information for current work

3. **Task implementation**: For each task in the section
   - Display: `## Implementation - 3.number.[Section Name]: [Task Title]`
   - **(Optional) High-Level Context Retrieval**: Use `codebase_search` targeting the `.cursor_memory` directory with keywords from the task to find relevant high-level vision or context notes that might inform the implementation.
   - Evaluate if the task is complex and requires in-depth reflection
   - If yes, use the think token: `<think>In-depth reflection using chain of thought, comparison of different implementation approaches, etc.</think>`
   - Briefly summarize the conclusions of the reflection if applicable

   - **Decision Point:** Based on the task analysis and reflection:
     - If the task primarily involves executing a complex command, script, or experiment (e.g., data processing, model training, performance test) AND requires analysis of the output → **Call the `experience-execution` rule instead of proceeding with direct implementation below.**
     - Otherwise (standard code writing/modification) → Proceed with the implementation steps below.

   - Implement the solution (Only if not calling `experience-execution`)
   - After each tool call, write **(Implementation - 3.number.[Section Name]: [Task Title] in progress...)**

4. **Task update**: Update tasks.md
   - Move section from "ToDo" to "In Progress" if necessary
   - Mark individual tasks as in progress

5. **Calling the next rule**: Mandatory to choose (Only if `experience-execution` was NOT called in step 3)
   - If `experience-execution` was called, its own step 8 dictates the next rule (`context-update`, `fix`, or `test-implementation`).
   - Otherwise, based on the implementation done in *this* rule:
     - IF testable feature created or modified → `experience-execution` (for manual testing and verification)
     - ELSE IF Only non-testable changes (e.g., documentation, rule edits) → `context-update`

## Specifics

- The think token `<think></think>` must be used for each complex individual task
- In-depth reflection must be verbalized in the think token before implementation
- Using the think token allows reasoning according to the chain of thought method and comparing different approaches
- Implement one task at a time, in logical order
- Comment complex parts of the code
- A feature is "testable" if it involves creating/modifying a script or function
- To avoid losing the workflow, systematically write **(Implementation - 3.number.[Section Name]: [Task Title] in progress...)** between each step

**VERY IMPORTANT : Using the Advanced MCP Terminal Tools:**

For executing shell commands, ALWAYS USE MyMCP and NEVER USE the terminal tool for better control and monitoring:
1.  **Launch:** Call the `mcp_execute_command` MCP tool with the `command` to run and an optional `timeout` in seconds. It returns the `pid` and initial output/status after the timeout (default 15s) or command completion, whichever comes first. The command continues running in the background if the MCP timeout is reached. I STRONGLY encourage using short timeouts, even to the point of monitoring command execution with increasingly longer timeouts via `mcp_get_terminal_status`. The idea is to be as efficient as possible and to be able to interrupt commands with minimal issue or in the face of an unusually long wait.
2.  **Check Status:** Call the `mcp_get_terminal_status` MCP tool with an optional `timeout` in seconds to get the list of all running/completed commands, their `pid`, `status` (`Running`, `Success`, `Failure`, `Stopped`), `exit_code`, and recent output. Providing a `timeout` waits for a status change.
3.  **Get Output:** Call the `mcp_get_terminal_output` MCP tool with the target `pid` and an optional `lines` count to retrieve the last N lines (default 100) of stdout/stderr for that process.
4.  **Stop & Cleanup:** Call the `mcp_stop_terminal_command` MCP tool with the target `pid` and an optional `lines` count to terminate a running command, retrieve its final output, and remove it from tracking.
This workflow allows managing long processes without blocking and retrieving intermediate/final results reliably.

- Always call the appropriate next rule

## Next Rules
- `experience-execution`: If new testable feature created or modified (DEFAULT for testing approach)
- `context-update`: ONLY if non-testable changes (pure documentation)

## Example

# Implementation: 1 - Task analysis
I begin by reading the tasks.md file to identify the highest priority section. **(Implementation: 1 - Task analysis)**
[...reading tasks.md file...]
**(Implementation: 1 - Task analysis)**

# Implementation: 2 - Active context update
I will update the active context to document what I will be working on. **(Implementation: 2 - Active context update)**
[...reading activeContext.md file...]
[...updating with relevant information for tasks to perform...]
[...adding information researched online...]
**(Implementation: 2 - Active context update)**

# Implementation: 3 - Task implementation
I will implement each task in the section following a methodical approach. **(Implementation: 3 - Task implementation)**

## Implementation - 3.1.Development: Script creation
*(Optional) I search for high-level context in `.cursor_memory` related to script patterns or goals: [...]*
`codebase_search(query='script creation goals patterns', target_directories=['.cursor/memory'])`
This task is complex and requires in-depth reflection.

<think>
To create this script, I have several architecture options:
1. A monolithic architecture where all logic is in a single file
2. A modular architecture with separation of concerns
3. An event-driven approach

Let's compare these approaches:
- Option 1: Simple to implement but difficult to maintain long-term
- Option 2: Better maintainability but initially more complex
- Option 3: Flexible but potentially over-optimized for this use case

I think option 2 is most appropriate as it offers a good balance between maintainability and initial complexity. I will therefore create:
- A main module for orchestration
- A module for API connection management
- A utility module for common functions

**Decision Point Analysis:** This task involves writing new code (modules, functions). Although some simple commands might be run during development or testing locally, the core activity is code creation, not executing a predefined complex experiment requiring specific output analysis. Therefore, I will proceed with implementation within this rule and not call `experience-execution`.
</think>

After reflection, I determined that the best approach is to use a modular architecture with separation of concerns. This task is primarily code modification/creation, not a complex experiment execution, so I proceed here.

I begin by creating the main script with the required functionalities. **(Implementation: 3.1 - Script creation)**
[...script implementation...]
**(Implementation - 3.1.Development: Script creation in progress...)**
[...implementation continuation...]
**(Implementation: 3.1 - Script creation)**

## Implementation - 3.2.Development: Documentation
*(Optional) I search for high-level context in `.cursor_memory` related to documentation standards or project vision: [...]*
`codebase_search(query='documentation standards vision', target_directories=['.cursor/memory'])`
I add the necessary documentation to explain how the script works. **(Implementation: 3.2 - Documentation)**
[...adding documentation...]
**(Implementation - 3.2.Development: Documentation in progress...)**
[...documentation finalization...]
**(Implementation: 3.2 - Documentation)**

# Implementation: 4 - Task update
I update the tasks.md file to reflect task progress. **(Implementation: 4 - Task update)**
[...updating tasks.md file...]
**(Implementation: 4 - Task update)**

# Implementation: 5 - Calling the next rule
I did not call `experience-execution`. Since I created a new testable feature (the script), I must now call the `experience-execution` rule.
The experience-execution rule must be called to create tests for the new feature. **(Implementation: 5 - Calling the next rule)**

[...calling tool 'fetch_rules' with rule_names=["experience-execution"]...]




[...] (The workflow must continue uninterrupted: even if the example stops here, the agent must IMPERATIVELY continue in reality: it MUST call and execute the following rule, then the next, etc., without ever stopping -> It is completely autonomous.)