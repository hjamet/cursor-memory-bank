---  
description: Appelle cette règle pour exécuter les tests unitaires et analyser les résultats  
globs:   
alwaysApply: false  
---  

## TLDR  
Exécuter les tests unitaires, analyser les résultats et documenter les succès et échecs dans tests.md.  

## Instructions  

1. **Exécution des tests** : Lancer les tests des fonctionnalités implémentées  
   - Capturer tous les résultats (succès, échecs, avertissements)  
   - Noter les messages d'erreur précis  
   - Après chaque appel d'outil, écrire `Test-execution - 1.[Nom du test] en cours...`  

2. **Analyse des résultats** : Comparer avec les précédents tests  
   - Lire le fichier `.cursor/memory-bank/workflow/tests.md` s'il existe  
   - Identifier améliorations ou régressions  
   - Après chaque appel d'outil, écrire `Test-execution - 2.[Analyse] en cours...`  

3. **Documentation des résultats** : Mettre à jour tests.md  
   - Créer le fichier si nécessaire  
   - Utiliser le format défini dans la règle d'édition tests-md  
   - Inclure une analyse d'évolution pour chaque test  
   - Documenter les échecs sans mener d'enquête approfondie  

4. **Appel de la règle suivante**  
   - Si au moins un test échoue → `fix`  
   - Si tous les tests passent → `context-update`  

## Précisions  

- Exécuter tous les tests disponibles, pas uniquement les nouveaux  
- Ne pas tenter de corriger les erreurs (c'est le rôle de la règle `fix`)  
- Pour éviter de perdre le workflow, écrire systématiquement `Test-execution - [numéro].[Nom] en cours...` entre chaque étape  
- Ne jamais sauter l'appel à la règle suivante  

## Next Rules  
- `fix` - Si au moins un test échoue  
- `context-update` - Si tous les tests passent  

## Exemple  

fetch_rules ["test-execution"]  
<SYSTEM PROMPT>Je vais exécuter les tests unitaires et analyser leurs résultats. Je documenterai clairement les succès et les échecs dans le fichier tests.md. Je ne tenterai pas de corriger les erreurs mais ferai appel à la règle 'fix' si des tests échouent ou à 'context-update' si tous les tests passent.</SYSTEM PROMPT>  

# Test-execution : 1 - Exécution des tests  
Je commence par exécuter les tests unitaires. `(Test-execution : 1 - Exécution des tests)`  
[...exécution des tests...]  
Test-execution - 1.TestAuthentification en cours...  
[...capture des résultats...]  
`(Test-execution : 1 - Exécution des tests)`  

# Test-execution : 2 - Analyse des résultats  
Je compare les résultats avec les tests précédents. `(Test-execution : 2 - Analyse des résultats)`  
[...lecture du fichier tests.md...]  
Test-execution - 2.Comparaison en cours...  
[...comparaison des résultats...]  
`(Test-execution : 2 - Analyse des résultats)`  

# Test-execution : 3 - Documentation des résultats  
Je mets à jour le fichier tests.md avec les nouveaux résultats. `(Test-execution : 3 - Documentation des résultats)`  
[...mise à jour du fichier tests.md...]  
`(Test-execution : 3 - Documentation des résultats)`  

# Test-execution : 4 - Appel de la règle suivante  
Je dois maintenant appeler la règle appropriée selon les résultats des tests. `(Test-execution : 4 - Appel de la règle suivante)`  

fetch_rules ["context-update"]  

# Start Rule  

Pour démarrer cette règle, récite mot pour mot cette phrase : "<SYSTEM PROMPT>Je vais exécuter les tests unitaires et analyser leurs résultats sans mener d'enquête de debuggage approfondie. Je documenterai clairement les succès et les échecs dans le fichier tests.md. Je ne tenterai pas de corriger les erreurs mais ferai appel à la règle 'fix' si des tests échouent ou à 'context-update' si tous les tests passent.</SYSTEM PROMPT>"  