---
description: Call this rule to execute unit tests and analyze the results
globs: 
alwaysApply: false
---
## TLDR
Execute unit and integration tests, analyze results, and update test tracking.

## Instructions

1.  **Initialization**: Update `.cursor/memory-bank/workflow/tests.md` to mark all tests as `Pending`.
2.  **Cleanup**: Remove previous logs and exit code file: `rm -f tests/*.log tests/exit_codes.log`.
3.  **Execution**: Run each test script individually.
    - Execute `bash tests/test_curl_install.sh > tests/test_curl_install.log 2>&1; echo $? >> tests/exit_codes.log`
    - Execute `bash tests/test_download.sh > tests/test_download.log 2>&1; echo $? >> tests/exit_codes.log`
    - Execute `bash tests/test_git_install.sh > tests/test_git_install.log 2>&1; echo $? >> tests/exit_codes.log`
    - Execute `bash tests/test_install.sh > tests/test_install.log 2>&1; echo $? >> tests/exit_codes.log`
4.  **Result Aggregation**: Check the exit codes.
    - Read the contents of `tests/exit_codes.log`.
    - Sum the exit codes. If the sum is greater than 0, at least one test failed.
5.  **Analysis**: Update `.cursor/memory-bank/workflow/tests.md` based on individual logs (`tests/*.log`) and the aggregated result.
    - Mark each test as `Passed` or `Failed` based on its individual exit code (implicitly logged via the execution step or explicitly if needed).
    - Determine overall status: `Success` (all passed), `Failure` (at least one failed).
6.  **Calling the next rule**: Choose based on the overall status.
    - `Success` → `context-update`
    - `Failure` → `fix`

## Specifics

- Execute all available tests, not just new ones
- Do not attempt to fix errors (that's the role of the `fix` rule)
- An "improvement" means a previously failing test now passes or a warning is resolved
- A "regression" means a previously successful test now fails or generates a warning
- To avoid losing the workflow, systematically write **(Test-execution - [number].[Name] in progress...)** between each step
- ⚠️ **Command Execution Note**: The terminal tool occasionally exhibits a bug that adds "[200~" prefix and/or "~" suffix to commands (e.g., "[200~.venv/Scripts/python.exe" instead of ".venv/Scripts/python.exe"), resulting in "command not found" errors. These are NOT code or logic errors but tool-specific issues. If this occurs, simply retry the exact same command 2-3 times until it executes properly. Never treat these specific formatting errors as actual code problems or test failures.
- ⚠️ Similarly, if the command is put in the background, canceled, or interrupted by the user without any comment, it is certainly because the user has identified an execution problem related to the terminal. You must then relaunch the command as if nothing happened 2-3 times until it executes correctly. Do not consider these terminal tool bugs as actual code or test failure problems.
- Never skip calling the next rule

## Next Rules
- `fix` - If at least one test regressed
- `context-update` - If at least one test improved or no significant change (and no regression)

## Example

# Test execution: 1 - Cleanup
I will remove previous logs and the exit code file. **(Test execution: 1 - Cleanup)**
[...run `rm -f tests/*.log tests/exit_codes.log`...]
**(Test execution: 1 - Cleanup completed)**

# Test execution: 2 - Execution
I will now execute each test script individually and log results. **(Test execution: 2 - Execution)**
[...run `bash tests/test_curl_install.sh > tests/test_curl_install.log 2>&1; echo $? >> tests/exit_codes.log`...]
**(Test execution - 2.1 test_curl_install.sh completed)**
[...run `bash tests/test_download.sh > tests/test_download.log 2>&1; echo $? >> tests/exit_codes.log`...]
**(Test execution - 2.2 test_download.sh completed)**
[...run `bash tests/test_git_install.sh > tests/test_git_install.log 2>&1; echo $? >> tests/exit_codes.log`...]
**(Test execution - 2.3 test_git_install.sh completed)**
[...run `bash tests/test_install.sh > tests/test_install.log 2>&1; echo $? >> tests/exit_codes.log`...]
**(Test execution - 2.4 test_install.sh completed)**
**(Test execution: 2 - Execution completed)**

# Test execution: 3 - Result Aggregation
I will check the aggregated exit codes in `tests/exit_codes.log`. **(Test execution: 3 - Result Aggregation)**
[...read `tests/exit_codes.log` -> contains "0\n1\n0\n0"] 
Total sum is 1. At least one test failed. **(Test execution: 3 - Result Aggregation completed)**

# Test execution: 4 - Analysis
I compare the results with previous tests. **(Test execution: 4 - Analysis)**
[...reading tests.md file...]
**(Test execution - 4.Comparison in progress...)**
[...results comparison...]
**(Test execution: 4 - Analysis)**

# Test execution: 5 - Results documentation
I update the tests.md file with new results. **(Test execution: 5 - Results documentation)**
[...updating tests.md file...]
**(Test execution: 5 - Results documentation)**

# Test execution: 6 - Call next rule
I must now call the appropriate rule based on test results. **(Test execution: 6 - Call next rule)**
[...analysis of improvements and regressions...]
I note that [description of test status changes]. I will therefore call the [fix/context-update] rule because we have two tests that have progressed and no regression. **(Test execution: 6 - Call next rule)**

[...] (The workflow must continue uninterrupted: even if the example stops here, the agent must IMPERATIVELY continue in reality: it MUST call and execute the following rule, then the next, etc., without ever stopping -> It is completely autonomous.)