---
description: Call this rule to analyze and correct test errors via an iterative execution-correction loop
globs: 
alwaysApply: false
---
## TLDR
Analyze test errors and fix them through an iterative execution-correction loop until all tests pass or truly complex issues (requiring major refactoring or showing no progress after 3 attempts) are identified and handed over to `request-analysis`.

## Instructions

1.  **Error identification**: Analyze and catalog test errors
    *   Read `.cursor/memory-bank/workflow/tests.md` to understand test results
    *   Establish a precise list of failing tests to address and their error messages
    *   Write **(Fix: 1 - Error identification completed)** after this step

2.  **Correction loop**: For each failing test, iterate until fixed or handed over
    *   **2.1 Marking**: Always indicate **(Fix: 2 - Correction loop - [Test name] - Iteration [number])**
    *   **2.2 Analysis**: 
        *   **(Optional) Memory Lookup**: Use `mcp_supermemory_searchSupermemory` with error message/test name to find similar past errors and their solutions.
        *   Examine only files involved in the error and identify the cause. If you are not entirely certain about the correction to be made. Don't hesitate to use the `<think></think>` tags. Don't hesitate either to use the codebase search tool!
    *   **2.3 Research**: 
        *   If needed, use the web to find solutions for specific errors.
        *   **Library Documentation Lookup**: If the error seems related to a specific library's usage (e.g., API misuse, unexpected behavior), use `mcp_context7_resolve-library-id` to get the library ID, followed by `mcp_context7_get-library-docs` to fetch relevant documentation for that library and the specific topic/function causing the error.
    *   **2.4 Implementation**: Make the correction and document the changes.
    *   **2.5 Execution**: Run the specific failing test script identified in the previous step to confirm if the fix works. Capture the exit code and output.
    *   **2.6 Loop analysis**: 
        *   If the test passes: *(Optional) Store successful fix details using `mcp_supermemory_addToSupermemory`*. Proceed to the next failing test or step 3 if all are fixed.
        *   If the test still fails after 3 attempts OR requires major refactoring (explain why in `<think>` tags): Hand over to `request-analysis`. Provide detailed context (problem description, files involved, failed attempts). Call `fetch_rules` for `request-analysis` and terminate this rule.
        *   If the test still fails but shows progress or the cause is unclear (and < 3 attempts): Continue the loop (go back to 2.2).
        *   **Advanced Debugging (If Stuck)**: If standard analysis and research fail to pinpoint the issue after an attempt, consider using the interactive debugger:
            1.  Use `mcp_debug_listFiles` to confirm relevant source file paths.
            2.  Use `mcp_debug_getFileContent` to review the code around the suspected error location.
            3.  Use `mcp_debug_debug` with `steps`:
                *   `[{ "type": "setBreakpoint", "file": "path/to/file.py", "line": <line_number> }]` to set breakpoints before the error occurs.
                *   `[{ "type": "launch" }]` to start the test execution under the debugger.
                *   Evaluate variables/expressions using `[{ "type": "evaluate", "expression": "variable_name" }]` while paused.
                *   Continue execution with `[{ "type": "continue" }]`.
                *   Repeat setting breakpoints, launching, evaluating, and continuing until the issue is understood.
            4.  Use the insights gained from debugging to inform the next implementation attempt (step 2.4).
        *   After this sub-step, write **(Fix - 2.6 Loop analysis completed)**

3.  **Calling the next rule (only if loop completed without handover)**: Determine the next step based on results *if and only if* step 2 did not trigger a handover to `request-analysis`.
    *   First check for unprocessed comments in `.cursor/memory-bank/userbrief.md`
    *   If unprocessed comments exist → call `consolidate-repo`
    *   If all initially failing tests were fixed → call `test-execution` for final verification
    *   Write **(Fix: 3 - Calling the next rule)**

## Specifics

-   The `<think></think>` token must be used for each complex correction requiring in-depth analysis
-   You should examine the files you know related to the problem but you should also use the codebase search tool in case you are missing something.
-   For basic debugging, temporarily adding logs is acceptable, but prefer the `mcp_debug` tools if you are stuck after an initial attempt.
-   NEVER execute all tests, but ONLY the specific test you are currently fixing
-   Running the complete test suite is exclusively the responsibility of the `test-execution` rule
-   Document ONLY errors related to:
    *   API changes in libraries
    *   Non-intuitive framework behaviors
    *   Updates that modify existing functionalities
-   CRUCIAL: To avoid losing workflow tracking, ALWAYS indicate the current test and iteration
-   ⚠️ **Command Execution Note**: The terminal tool occasionally exhibits a bug that adds "[200~" prefix and/or "~" suffix to commands (e.g., "[200~.venv/Scripts/python.exe" instead of ".venv/Scripts/python.exe"), resulting in "command not found" errors. These are NOT code or logic errors but tool-specific issues. If this occurs, simply retry the exact same command 2-3 times until it executes properly. Never treat these specific formatting errors as actual code problems or test failures.
-   ⚠️ Similarly, if the command is put in the background, canceled, or interrupted by the user without any comment, it is certainly because the user has identified an execution problem related to the terminal. You must then relaunch the command as if nothing happened 2-3 times until it executes correctly. Do not consider these terminal tool bugs as actual code or test failure problems.

## Next Rules
-   `consolidate-repo` - If loop completes and unprocessed comments exist
-   `test-execution` - If loop completes and all initially failing tests were fixed
-   `request-analysis` - Called directly from step 2.5 for truly complex issues, terminating this rule's execution.

## Example

[...fetching rule `fix`...] # Use your rule-calling tool to call the rule, then apply it.

# Fix: 1 - Error identification
I begin by reading the tests.md file to understand test errors. **(Fix: 1 - Error identification)**
[...reading .cursor/memory-bank/workflow/tests.md file...]

I've identified the following failing tests:
1.  TestUserAuthentication.test_login_with_invalid_token - TypeError: Object of type bytes is not JSON serializable
2.  TestDataProcessing.test_large_file_processing - MemoryError: Unable to allocate memory for operation
3.  TestAPIConnection.test_retry_mechanism - AssertionError: Expected 3 retry attempts, got 0
**(Fix: 1 - Error identification completed)**

# Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.2 Analysis
*(Optional) I search my memory for similar errors: [...]*
`mcp_supermemory_searchSupermemory(...)`
[...examining authentication code files...]

<think>
The error occurs because we're trying to JSON serialize a bytes object. Decoding to 'utf-8' before serialization should work.
</think>

## 2.3 Research
[...confirming JSON serialization practices...]
*(If a library issue was suspected)*
I need to check the documentation for the `json` library regarding byte serialization.
`mcp_context7_resolve-library-id(libraryName='json')`
[...]
`mcp_context7_get-library-docs(context7CompatibleLibraryID='...', topic='serialization')`

## 2.4 Implementation
[...modifying the authentication code to decode bytes before JSON serialization...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Implementation 1 completed)**

## 2.5 Execution
I will run the failing test script (`tests/test_install.sh`) to see if the fix works. **(Fix - 2.5 Execution in progress...)**
[...run `bash tests/test_install.sh > tests/install.log 2>&1; echo $? > tests/install.exitcode`...]
**(Fix - 2.5 Execution completed)**

## 2.6 Loop analysis
Great! The test now passes. 
*(Optional) I store the details of the fix in my memory.*
`mcp_supermemory_addToSupermemory(...)`
Moving to the next failing test.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.2 Analysis
[...examining data processing code...]
<think>
MemoryError indicates loading the whole file. A streaming approach is likely needed.
</think>

## 2.3 Research
[...searching for streaming file processing techniques...]

## 2.4 Implementation
[...modifying the processor to use a basic streaming approach...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 1 completed)**

## 2.5 Execution
I will run the failing test script (`tests/test_install.sh`) to see if the fix works. **(Fix - 2.5 Execution in progress...)**
[...run `bash tests/test_install.sh > tests/install.log 2>&1; echo $? > tests/install.exitcode`...]
**(Fix - 2.5 Execution completed)**

## 2.6 Loop analysis
The test fails with "NameError: name 'process_chunk' is not defined". This shows progress, so I'll continue iterating. **(Fix - 2.6 Loop analysis completed)**

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.2 Analysis
[...analyzing the NameError...]

## 2.3 Research
[...]

## 2.4 Implementation
[...implementing the missing function...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 2 completed)**

## 2.5 Execution
I will run the failing test script (`tests/test_install.sh`) to see if the fix works. **(Fix - 2.5 Execution in progress...)**
[...run `bash tests/test_install.sh > tests/install.log 2>&1; echo $? > tests/install.exitcode`...]
**(Fix - 2.5 Execution completed)**

## 2.6 Loop analysis
I see that the command has been put in the background. It's certainly related to a simple terminal issue. I'm going to relaunch the command. **(Fix - 2.6 Loop analysis completed)**
[...re-running the test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
Still failing with a MemoryError. The simple streaming wasn't enough.

**(Advanced Debugging Example)**
I'm stuck on this MemoryError. I will use the debugger. **(Fix - 2.6 Loop analysis completed)**
`mcp_debug_listFiles(includePatterns=['**/data_processor.py'])`
[...]
`mcp_debug_getFileContent(path='path/to/data_processor.py')`
[...]
`mcp_debug_debug(steps=[{'type': 'setBreakpoint', 'file': 'path/to/data_processor.py', 'line': 42}])`
[...]
`mcp_debug_debug(steps=[{'type': 'launch'}])`
[...] (Paused at breakpoint)
`mcp_debug_debug(steps=[{'type': 'evaluate', 'expression': 'current_chunk_size'}])`
[...]
`mcp_debug_debug(steps=[{'type': 'continue'}])`
[...] (Debugger finished or hit another breakpoint)
Now I understand the issue better and will try a new implementation.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.2 Analysis
[...re-evaluating the memory issue...]
<think>
After three attempts, the MemoryError persists despite initial fixes. The first attempt failed with MemoryError, the second with NameError (progress), but the third attempt reverted to MemoryError. This indicates the simple streaming approach is insufficient and likely requires a significant refactoring, possibly involving buffered reading, memory-mapped files, or an external library designed for large datasets. This level of change qualifies as a major refactoring and has shown no consistent progress toward resolution after 3 tries. Handing over is appropriate.
</think>

## 2.3 Research
[...]

## 2.4 Implementation
[...No further implementation attempt...]

## 2.5 Execution
I will run the failing test script (`tests/test_install.sh`) to see if the fix works. **(Fix - 2.5 Execution in progress...)**
[...run `bash tests/test_install.sh > tests/install.log 2>&1; echo $? > tests/install.exitcode`...]
**(Fix - 2.5 Execution completed)**

## 2.6 Loop analysis
The test failed on the previous iteration, and analysis confirms it needs major refactoring beyond simple iteration.
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Handing over to request-analysis)**
**(Fix - 2.6 Loop analysis completed)**

Calling `request-analysis` to handle the complex task of implementing efficient large file processing.
[Provide context: Problem="MemoryError during large file processing requires major refactoring (streaming/chunking insufficient)", Files=[...], Analysis="Initial attempts failed, needs new architectural approach."]

[...Here, the agent calls the rule 'request-analysis' using its rule calling tool...]

[...] (The workflow must continue uninterrupted: even if the example stops here, the agent must IMPERATIVELY continue in reality: it MUST call and execute the following rule, then the next, etc., without ever stopping -> It is completely autonomous.)

