---
description: Call this rule to analyze and correct test errors via an iterative execution-correction loop
globs: 
alwaysApply: false
---
## TLDR
Analyze test errors and fix them through an iterative execution-correction loop until all tests pass or truly complex issues (requiring major refactoring or showing no progress after 3 attempts) are identified and handed over to `request-analysis`.

## Instructions

1.  **Error identification**: Analyze and catalog test errors
    *   Read `.cursor/memory-bank/workflow/tests.md` to understand test results
    *   Establish a precise list of failing tests to address and their error messages
    *   Write **(Fix: 1 - Error identification completed)** after this step

2.  **Correction loop**: For each failing test, iterate until fixed or handed over
    *   **2.1 Marking**: Always indicate **(Fix: 2 - Correction loop - [Test name] - Iteration [number])**
    *   **2.2 Analysis**: 
        *   **(Optional) Memory Lookup**: Use `mcp_servers_search_nodes` with error message/test name to find similar past errors and their solutions in the knowledge graph.
        *   Examine only files involved in the error and identify the cause. If you are not entirely certain about the correction to be made. Don't hesitate to use the `<think></think>` tags. Don't hesitate either to use the codebase search tool !
    *   **2.3 Research**: If needed, use the web to find solutions for specific errors
    *   **2.4 Implementation**: Make the correction and document the changes
    *   **2.5 Test**: Execute ONLY the specific test currently being fixed and analyze the result:
        *   If test passes :
            * **2.5.1 Update tests.md**: If the test passes, update the `.cursor/memory-bank/workflow/tests.md` file to reflect the new status of the test. Write **(Fix: 2 - Correction loop - [Test name] - Implementation [number] completed)** after this step.
            * **2.5.2 Quick commit** : Use the command `git commit -m "Message" files` to commit the changes made to the files involved in the test. The message should look like that : **:wrench: fix: test_name - quick fix explanation**.
            * **(Optional) Memory Storage**: Use `mcp_servers_add_observations` to store details of the error, the successful fix, and relevant context (e.g., files modified, iteration count) in the knowledge graph, associated with an entity like 'TestErrorSolution'.
            * **2.5.3 Continue**: Move to the next failing test in the list and repeat the loop from step 2.1.
        *   If test fails but shows progress → Continue loop with new iteration for the *same* test.
        *   If **truly complex correction** (after 3 attempts *without any progress* OR requires *major* refactoring / a completely new implementation approach):
            *   Mark the test as **(Fix: 2 - Correction loop - [Test name] - Handing over to request-analysis)**
            *   Call the `request-analysis` rule, providing the problem description, context, and the need for a new approach.

3.  **Calling the next rule (only if loop completed without handover)**: Determine the next step based on results *if and only if* step 2.5 did not trigger a handover to `request-analysis`.
    *   First check for unprocessed comments in `.cursor/memory-bank/userbrief.md`
    *   If unprocessed comments exist → call `consolidate-repo`
    *   If all initially failing tests were fixed → call `test-execution` for final verification
    *   Write **(Fix: 3 - Calling the next rule)**

## Specifics

-   The `<think></think>` token must be used for each complex correction requiring in-depth analysis
-   You should examinate the files you know related to the problem but you should also use the codebase search tool in case you are missing something.
-   For debugging, temporarily add logs at strategic locations
-   NEVER execute all tests, but ONLY the specific test you are currently fixing
-   Running the complete test suite is exclusively the responsibility of the `test-execution` rule
-   Document ONLY errors related to:
    *   API changes in libraries
    *   Non-intuitive framework behaviors
    *   Updates that modify existing functionalities
-   CRUCIAL: To avoid losing workflow tracking, ALWAYS indicate the current test and iteration
-   ⚠️ **Command Execution Note**: The terminal tool occasionally exhibits a bug that adds "[200~" prefix and/or "~" suffix to commands (e.g., "[200~.venv/Scripts/python.exe" instead of ".venv/Scripts/python.exe"), resulting in "command not found" errors. These are NOT code or logic errors but tool-specific issues. If this occurs, simply retry the exact same command 2-3 times until it executes properly. Never treat these specific formatting errors as actual code problems or test failures.
-   ⚠️ Similarly, if the command is put in the background, canceled, or interrupted by the user without any comment, it is certainly because the user has identified an execution problem related to the terminal. You must then relaunch the command as if nothing happened 2-3 times until it executes correctly. Do not consider these terminal tool bugs as actual code or test failure problems.

## Next Rules
-   `consolidate-repo` - If loop completes and unprocessed comments exist
-   `test-execution` - If loop completes and all initially failing tests were fixed
-   `request-analysis` - Called directly from step 2.5 for truly complex issues, terminating this rule's execution.

## Example

[...fetching rule `fix`...] # Use your rule-calling tool to call the rule, then apply it.

# Fix: 1 - Error identification
I begin by reading the tests.md file to understand test errors. **(Fix: 1 - Error identification)**
[...reading .cursor/memory-bank/workflow/tests.md file...]

I've identified the following failing tests:
1.  TestUserAuthentication.test_login_with_invalid_token - TypeError: Object of type bytes is not JSON serializable
2.  TestDataProcessing.test_large_file_processing - MemoryError: Unable to allocate memory for operation
3.  TestAPIConnection.test_retry_mechanism - AssertionError: Expected 3 retry attempts, got 0
**(Fix: 1 - Error identification completed)**

# Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.2 Analysis
*(Optional) I search the knowledge graph for similar errors: [...]*
`mcp_servers_search_nodes(...)`
[...examining authentication code files...]

<think>
The error occurs because we're trying to JSON serialize a bytes object. Decoding to 'utf-8' before serialization should work.
</think>

## 2.3 Research
[...confirming JSON serialization practices...]

## 2.4 Implementation
[...modifying the authentication code to decode bytes before JSON serialization...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Implementation 1 completed)**

## 2.5 Test
[...running the specific authentication test...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Test execution 1)**

Great! The test now passes. 
*(Optional) I store the details of the fix in the knowledge graph.*
`mcp_servers_add_observations(...)`
Moving to the next failing test.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.2 Analysis
[...examining data processing code...]
<think>
MemoryError indicates loading the whole file. A streaming approach is likely needed.
</think>

## 2.3 Research
[...searching for streaming file processing techniques...]

## 2.4 Implementation
[...modifying the processor to use a basic streaming approach...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 1 completed)**

## 2.5 Test
[...running the specific data processing test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 1)**

The test fails with "NameError: name 'process_chunk' is not defined". This shows progress, so I'll continue iterating.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.2 Analysis
[...analyzing the NameError...]

## 2.3 Research
[...]

## 2.4 Implementation
[...implementing the missing function...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 2 completed)**

## 2.5 Test
[...running the test again...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
I see that the command has been put in the background. It's certainly related to a simple terminal issue. I'm going to relaunch the command.
[...re-running the test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
Still failing with a MemoryError. The simple streaming wasn't enough.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.2 Analysis
[...re-evaluating the memory issue...]
<think>
After three attempts, the MemoryError persists despite initial fixes. The first attempt failed with MemoryError, the second with NameError (progress), but the third attempt reverted to MemoryError. This indicates the simple streaming approach is insufficient and likely requires a significant refactoring, possibly involving buffered reading, memory-mapped files, or an external library designed for large datasets. This level of change qualifies as a major refactoring and has shown no consistent progress toward resolution after 3 tries. Handing over is appropriate.
</think>

## 2.3 Research
[...]

## 2.4 Implementation
[...No further implementation attempt...]

## 2.5 Test
The test failed on the previous iteration, and analysis confirms it needs major refactoring beyond simple iteration.
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Handing over to request-analysis)**

Calling `request-analysis` to handle the complex task of implementing efficient large file processing.
[Provide context: Problem="MemoryError during large file processing requires major refactoring (streaming/chunking insufficient)", Files=[...], Analysis="Initial attempts failed, needs new architectural approach."]

[...fetching rule `request-analysis`...] # Use your rule-calling tool to call the rule, then apply it !

