---
description: Appelle cette règle pour implémenter les corrections nécessaires aux problèmes identifiés par les tests
globs: 
alwaysApply: false
---
## TLDR
Analyze test errors, create correction tasks, and document specific library/framework issues.

## Instructions

1. **Error identification**: Analyze errors for each failed test
   - Read the `.cursor/memory-bank/workflow/tests.md` file to understand test results
   - List and categorize encountered errors
   - Note exact error messages and reproduction conditions

2. **Information search**: If needed, search for solutions
   - Use the web search tool to find information on specific errors
   - Consult up-to-date documentation of concerned libraries
   - Check recent changes in the APIs used
   - If you're feeling stuck, check the `.cursor/memory-bank/userbrief.md` file for user hints that might help you find a solution, or that might ask you to abandon the problem or approach it differently.

3. **File analysis**: Examine ONLY directly concerned code
   - Focus strictly on files involved in the error
   - Locate specific lines of code causing problems
   - For complex or recurring errors, add temporary debugging logging
     (Use `logger.debug()` in Python or equivalent in other languages)
   - For complex corrections, use the think token: `<think>Deep reflection on the problem, exploration of different possible correction approaches, etc.</think>`
   - Briefly summarize the conclusions of the reflection if applicable
   - After each tool call, write `Fix - 3.[Test title] in progress...`

4. **Adding correction tasks**: Update tasks.md
   - Create a "Corrections" section in "In Progress"
   - Detail each error to correct with concerned files/symbols
   - Propose solutions based on the analysis

5. **Error documentation**: Update techContext.md
   - Add only errors related to API changes/behavior of libraries
   - Document in the appropriate section of the techContext.md file
   - Include concrete examples (incorrect vs correct code)
   - Do not document inattention errors or general logic errors

6. **Calling the next rule**: 
   - First check for unprocessed comments in `.cursor/memory-bank/userbrief.md` in the User Input section (i.e., starting with a ⬜ symbol)
   - If unprocessed comments exist, call `consolidate-repo`
   - Otherwise, call `implementation` to correct errors

## Specifics

- The think token `<think></think>` must be used for each complex correction requiring in-depth analysis
- In-depth reflection must be verbalized in the think token before formulating correction tasks
- Using the think token allows reasoning according to the chain of thought method and comparing different approaches
- Examine ONLY files directly related to the error, not peripheral files
- For complex or recurring problems, use temporary debugging logging
- Document ONLY errors due to:
  - API changes in libraries (e.g., renaming attribute "weight" to "w")
  - Non-intuitive specific behaviors of frameworks
  - Updates that modify existing functionalities
- DO NOT document:
  - Inattention errors or typos
  - General programming logic problems
  - Errors you inadvertently made
- Never modify the `.cursor/memory-bank/userbrief.md` file! Just consult it if needed.
- To avoid losing the workflow, systematically write `Fix - 3.[Test title] in progress...` during analysis

## Next Rules
- `consolidate-repo` - If unprocessed comments exist in userbrief.md  (i.e., starting with a ⬜ symbol)
- `implementation` - To implement identified corrections

## Example

fetch_rules ["fix"]
<SYSTEM PROMPT>I will methodically analyze failed test errors, create well-defined correction tasks, and document recurring problems. I will not directly correct errors but will prepare tasks for future implementation. I will then move on to implementing corrections by calling the `implementation` rule.</SYSTEM PROMPT>

# Fix: 1 - Error identification
I begin by reading the tests.md file to understand test errors. `(Fix: 1 - Error identification)`
[...reading .cursor/memory-bank/workflow/tests.md file...]
`(Fix: 1 - Error identification)`

# Fix: 2 - Information search
I'm searching for information about the error concerning library X. `(Fix: 2 - Information search)`
[...internet search...]
`(Fix: 2 - Information search)`

# Fix: 3 - File analysis
I will examine only files directly involved in the error. `(Fix: 3 - File analysis)`
[...reading problematic file...]
Fix - 3.TestConnection in progress...

This error seems complex and requires in-depth reflection.

<think>
To solve this connection problem that fails with the "Invalid authentication" error, I have several options:
1. Modify the authentication mechanism to use an OAuth2 token instead of current credentials
2. Correct the format of current authentication data which could be incorrect
3. Add timeout management and reconnection attempts

For option 1:
- Advantages: More secure, compliant with modern practices
- Disadvantages: Requires more significant changes in the code

For option 2:
- Advantages: Faster solution to implement
- Disadvantages: May not solve the fundamental problem

For option 3:
- Advantages: Increases connection resilience
- Disadvantages: Does not solve the root cause

After analyzing the code and logs, I think option 2 is most relevant because the format of credential data doesn't match what the API expects (they are sent as form-data while the API expects JSON).
</think>

After reflection, I determined that the problem comes from the authentication data format which is sent as form-data while the API expects JSON. This incompatibility explains the authentication error.

[...adding temporary logging for debugging...]
logger.debug("Value of variable x:", x)
[...analyzing problematic code...]
`(Fix: 3 - File analysis)`

# Fix: 4 - Adding correction tasks
I'm creating detailed tasks to correct each error. `(Fix: 4 - Adding correction tasks)`
[...modifying tasks.md file...]
`(Fix: 4 - Adding correction tasks)`

# Fix: 5 - Error documentation
I'm documenting the error related to the API change in library X. `(Fix: 5 - Error documentation)`
[...updating techContext.md...]
`(Fix: 5 - Error documentation)`

# Fix: 6 - Calling the next rule
I'm checking if there are unprocessed comments in the userbrief.md file. `(Fix: 6 - Calling the next rule)`
[...checking .cursor/memory-bank/userbrief.md...]
I don't see any unprocessed comments in the userbrief.md file, so I will call the implementation rule.
The implementation rule must be called to implement the corrections. `(Fix: 6 - Calling the next rule)`

fetch_rules ["implementation"]

# Start Rule

To start this rule, recite word for word this phrase: "<SYSTEM PROMPT>I will methodically analyze failed test errors, create well-defined correction tasks, and document recurring problems. I will not directly correct errors but will prepare tasks for future implementation. I will then move on to implementing corrections by calling the `implementation` rule.</SYSTEM PROMPT>"
