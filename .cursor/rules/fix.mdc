---
description: Call this rule to analyze and correct test errors via an iterative execution-correction loop
globs: 
alwaysApply: false
---
## TLDR
Analyze test errors and fix them through an iterative execution-correction loop until all tests pass or truly complex issues (requiring major refactoring or showing no progress after 3 attempts) are identified and handed over to `request-analysis`.

## Instructions

1.  **Error identification**: Analyze and catalog test errors
    *   Read `.cursor/memory-bank/workflow/tests.md` to understand test results
    *   Establish a precise list of failing tests to address and their error messages
    *   Write **(Fix: 1 - Error identification completed)** after this step

2.  **Correction loop**: For each failing test, iterate until fixed or handed over
    *   **2.1 Marking**: Always indicate **(Fix: 2 - Correction loop - [Test name] - Iteration [number])**
    *   **2.2 Analysis**: 
        *   **(Optional) Memory Lookup**: Use `mcp_memory_search_nodes` with query based on error message/test name to find similar past errors/solutions.
        *   Examine files involved, identify cause. Use `<think>` tags for complex analysis. Use codebase search if needed.
        *   **Decision Point**: Are you certain about the cause and the fix? If NO, plan to use debugging tools in step 2.4.
    *   **2.3 Research**: 
        *   If needed, use web search (`web_search`) for general solutions.
        *   **Library Documentation**: If the error seems library-related, use `mcp_context7_resolve-library-id` then `mcp_context7_get-library-docs`.
    *   **2.4 Implementation & Debugging**: 
        *   **Proactive Debugging (If Uncertain)**: If analysis (2.2) indicated uncertainty, use `mcp_debug` tools *now* to investigate before finalizing the implementation attempt:
            1. `mcp_debug_listFiles`
            2. `mcp_debug_getFileContent`
            3. `mcp_debug_debug` (set breakpoints, launch, evaluate, continue)
            4. Use insights gained to refine the fix.
        *   **Implementation**: Make the correction based on analysis, research, and potential debugging insights. Document changes.
    *   **2.5 Test**: Execute ONLY the specific test currently being fixed and analyze the result:
        *   If test passes:
            * **2.5.1 Update tests.md**: Update `.cursor/memory-bank/workflow/tests.md`. Write **(Fix: 2 - Correction loop - [Test name] - Implementation [number] completed)**.
            * **2.5.2 Quick commit**: Use `git commit -a -m ":wrench: fix: [Test name] - [quick fix explanation]"`.
            * **(Optional) Memory Storage**: 
                *   Ensure entity exists: `mcp_memory_create_entities(entities=[{"entityType": "FixLog", "name": "TestErrorSolutions", "observations": []}])` (ignore errors if exists).
                *   Add observation: `mcp_memory_add_observations(observations=[{"entityName": "TestErrorSolutions", "contents": ["Fixed [Test name]... Solution: ... Iteration: ..."]}])`.
            * **2.5.3 Continue**: Move to the next failing test.
        *   If test fails but shows progress → Continue loop (go to 2.1 for next iteration).
        *   If test fails after 3 attempts *without progress* OR requires *major* refactoring:
            *   Mark as **(Fix: 2 - Correction loop - [Test name] - Handing over to request-analysis)**. Call `request-analysis` with context.

3.  **Calling the next rule (only if loop completed without handover)**: Determine the next step based on results *if and only if* step 2.5 did not trigger a handover to `request-analysis`.
    *   First check for unprocessed comments in `.cursor/memory-bank/userbrief.md`
    *   If unprocessed comments exist → call `consolidate-repo`
    *   If all initially failing tests were fixed → call `test-execution` for final verification
    *   Write **(Fix: 3 - Calling the next rule)**

## Specifics

-   The `<think></think>` token must be used for each complex correction requiring in-depth analysis
-   You should examinate the files you know related to the problem but you should also use the codebase search tool in case you are missing something.
-   For basic debugging, temporarily adding logs is acceptable, but prefer the proactive use of `mcp_debug` tools as described in step 2.4 if uncertain about the error cause.
-   NEVER execute all tests, but ONLY the specific test you are currently fixing
-   Running the complete test suite is exclusively the responsibility of the `test-execution` rule
-   Document ONLY errors related to:
    *   API changes in libraries
    *   Non-intuitive framework behaviors
    *   Updates that modify existing functionalities
-   CRUCIAL: To avoid losing workflow tracking, ALWAYS indicate the current test and iteration
-   ⚠️ **Command Execution Note**: The terminal tool occasionally exhibits a bug that adds "[200~" prefix and/or "~" suffix to commands (e.g., "[200~.venv/Scripts/python.exe" instead of ".venv/Scripts/python.exe"), resulting in "command not found" errors. These are NOT code or logic errors but tool-specific issues. If this occurs, simply retry the exact same command 2-3 times until it executes properly. Never treat these specific formatting errors as actual code problems or test failures.
-   ⚠️ Similarly, if the command is put in the background, canceled, or interrupted by the user without any comment, it is certainly because the user has identified an execution problem related to the terminal. You must then relaunch the command as if nothing happened 2-3 times until it executes correctly. Do not consider these terminal tool bugs as actual code or test failure problems.

## Next Rules
-   `consolidate-repo` - If loop completes and unprocessed comments exist
-   `test-execution` - If loop completes and all initially failing tests were fixed
-   `request-analysis` - Called directly from step 2.5 for truly complex issues, terminating this rule's execution.

## Example

[...fetching rule `fix`...] # Use your rule-calling tool to call the rule, then apply it.

# Fix: 1 - Error identification
I begin by reading the tests.md file to understand test errors. **(Fix: 1 - Error identification)**
[...reading .cursor/memory-bank/workflow/tests.md file...]

I've identified the following failing tests:
1.  TestUserAuthentication.test_login_with_invalid_token - TypeError: Object of type bytes is not JSON serializable
2.  TestDataProcessing.test_large_file_processing - MemoryError: Unable to allocate memory for operation
3.  TestAPIConnection.test_retry_mechanism - AssertionError: Expected 3 retry attempts, got 0
**(Fix: 1 - Error identification completed)**

# Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.2 Analysis
*(Optional) I search memory for similar errors: [...]*
`mcp_memory_search_nodes(query='TypeError JSON serializable TestUserAuthentication')`
[...examining authentication code files...]

<think>
The error occurs because we're trying to JSON serialize a bytes object. Decoding to 'utf-8' before serialization should work.
I am reasonably certain about this fix.
</think>
**(Decision: Certainty=High, Debugging not needed immediately)**

## 2.3 Research
[...confirming JSON serialization practices...]
*(If library issue suspected)*
`mcp_context7_resolve-library-id(...)`
`mcp_context7_get-library-docs(...)`

## 2.4 Implementation & Debugging
[...modifying the authentication code to decode bytes before JSON serialization...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Implementation 1 completed)**

## 2.5 Test
[...running the specific authentication test...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Test execution 1)**

Great! The test now passes. 
*(Optional) Ensure entity exists.*
`mcp_memory_create_entities(entities=[{"entityType": "FixLog", "name": "TestErrorSolutions", "observations": []}])`
*(Optional) I store the details of the fix in memory.*
`mcp_memory_add_observations(observations=[{"entityName": "TestErrorSolutions", "contents": ["Fixed TestUserAuthentication... Solution: Decoded bytes... Iteration: 1."]}])`
Moving to the next failing test.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.2 Analysis
[...examining data processing code...]
<think>
MemoryError indicates loading the whole file. A streaming approach is likely needed, but the exact implementation detail causing the issue isn't immediately obvious without seeing the runtime state.
</think>
**(Decision: Certainty=Medium, Debugging recommended before/during implementation)**

## 2.3 Research
[...searching for streaming file processing techniques...]

## 2.4 Implementation & Debugging
**(Proactive Debugging)**
Since I'm uncertain about the exact cause, I'll use the debugger first.
`mcp_debug_listFiles(...)`
`mcp_debug_getFileContent(...)`
`mcp_debug_debug(steps=[... set breakpoint ...])`
`mcp_debug_debug(steps=[{'type': 'launch'}])`
`mcp_debug_debug(steps=[... evaluate variables ...])`
`mcp_debug_debug(steps=[{'type': 'continue'}])`
<think> Debugging revealed [specific finding]. This confirms a streaming approach is needed, focusing on [specific detail]. </think>
**(Implementation)**
Now, based on debugging insights, I will implement the streaming approach.
[...modifying the processor to use streaming...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 1 completed)**

## 2.5 Test
[...running the specific data processing test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 1)**

The test fails with "NameError: name 'process_chunk' is not defined". This shows progress, so I'll continue iterating.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.2 Analysis
[...analyzing the NameError...]

## 2.3 Research
[...]

## 2.4 Implementation
[...implementing the missing function...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 2 completed)**

## 2.5 Test
[...running the test again...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
I see that the command has been put in the background. It's certainly related to a simple terminal issue. I'm going to relaunch the command.
[...re-running the test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
Still failing with a MemoryError. The simple streaming wasn't enough.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.2 Analysis
[...re-evaluating the memory issue...]
<think>
After three attempts, the MemoryError persists despite initial fixes. The first attempt failed with MemoryError, the second with NameError (progress), but the third attempt reverted to MemoryError. This indicates the simple streaming approach is insufficient and likely requires a significant refactoring, possibly involving buffered reading, memory-mapped files, or an external library designed for large datasets. This level of change qualifies as a major refactoring and has shown no consistent progress toward resolution after 3 tries. Handing over is appropriate.
</think>

## 2.3 Research
[...]

## 2.4 Implementation
[...No further implementation attempt...]

## 2.5 Test
The test failed on the previous iteration, and analysis confirms it needs major refactoring beyond simple iteration.
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Handing over to request-analysis)**

Calling `request-analysis` to handle the complex task of implementing efficient large file processing.
[Provide context: Problem="MemoryError during large file processing requires major refactoring (streaming/chunking insufficient)", Files=[...], Analysis="Initial attempts failed, needs new architectural approach."]

[...Here, the agent calls the rule 'request-analysis' using its rule calling tool...]

[...] (The workflow must continue uninterrupted: even if the example stops here, the agent must IMPERATIVELY continue in reality: it MUST call and execute the following rule, then the next, etc., without ever stopping -> It is completely autonomous.)

