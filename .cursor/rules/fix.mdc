---  
description: Appelle cette règle pour implémenter les corrections nécessaires aux problèmes identifiés par les tests  
globs:   
alwaysApply: false  
---  

## TLDR  
Analyser les erreurs de test, créer des tâches de correction et documenter les problèmes spécifiques aux bibliothèques/frameworks.  

## Instructions  

1. **Identification des erreurs** : Analyser les erreurs pour chaque test échoué  
   - Lire le fichier `.cursor/memory-bank/workflow/tests.md` pour comprendre les résultats des tests  
   - Lister et catégoriser les erreurs rencontrées  
   - Noter les messages d'erreur exacts et les conditions de reproduction  

2. **Recherche d'informations** : Au besoin, rechercher des solutions  
   - Utiliser l'outil de recherche web pour trouver des informations sur des erreurs spécifiques  
   - Consulter la documentation à jour des bibliothèques concernées  
   - Vérifier les changements récents dans les APIs utilisées  

3. **Analyse des fichiers** : Examiner UNIQUEMENT le code directement concerné  
   - Se concentrer strictement sur les fichiers impliqués dans l'erreur  
   - Localiser les lignes de code spécifiques causant les problèmes  
   - Pour les erreurs complexes ou récurrentes, ajouter du logging temporaire de débogage  
     (Utiliser `logger.debug()` en Python ou équivalent dans d'autres langages)  
   - Après chaque appel d'outil, écrire `Fix - 3.[Titre du test] en cours...`  

4. **Ajout des tâches de correction** : Mettre à jour tasks.md  
   - Créer une section "Corrections" dans "In Progress"  
   - Détailler chaque erreur à corriger avec fichiers/symboles concernés  
   - Proposer des solutions basées sur l'analyse  

5. **Documentation des erreurs** : Mettre à jour techContext.md  
   - Ajouter uniquement les erreurs liées aux changements d'API/comportement des bibliothèques  
   - Documenter dans la section appropriée du fichier techContext.md  
   - Inclure des exemples concrets (code incorrect vs correct)  
   - Ne pas documenter les erreurs d'inattention ou de logique générale  

6. **Appel de la règle suivante** : Appeler `implementation` pour corriger les erreurs  

## Précisions  

- Examiner UNIQUEMENT les fichiers directement liés à l'erreur, pas les fichiers périphériques  
- Pour les problèmes complexes ou récurrents, utiliser du logging de débogage temporaire  
- Documenter UNIQUEMENT les erreurs dues à:  
  - Changements d'API dans les bibliothèques (ex: renommage d'attribut "weight" à "w")  
  - Comportements spécifiques non intuitifs de frameworks  
  - Mises à jour qui modifient les fonctionnalités existantes  
- NE PAS documenter:  
  - Erreurs d'inattention ou fautes de frappe  
  - Problèmes de logique générale de programmation  
  - Erreurs que tu as commises par inadvertance  
- Pour éviter de perdre le workflow, écrire systématiquement `Fix - 3.[Titre du test] en cours...` pendant l'analyse  

## Next Rules  
- `implementation` - Pour implémenter les corrections identifiées  

## Exemple  

fetch_rules ["fix"]  
<SYSTEM PROMPT>Je vais analyser méthodiquement les erreurs des tests échoués, créer des tâches de correction bien définies et documenter les problèmes récurrents. Je ne corrigerai pas directement les erreurs mais je préparerai les tâches pour l'implémentation future. Je passerai ensuite à l'implémentation des corrections en appellant la règle `implementation`</SYSTEM PROMPT>  

# Fix : 1 - Identification des erreurs  
Je commence par lire le fichier tests.md pour comprendre les erreurs des tests. `(Fix : 1 - Identification des erreurs)`  
[...lecture du fichier .cursor/memory-bank/workflow/tests.md...]  
`(Fix : 1 - Identification des erreurs)`  

# Fix : 2 - Recherche d'informations  
Je recherche des informations sur l'erreur concernant la bibliothèque X. `(Fix : 2 - Recherche d'informations)`  
[...recherche sur internet...]  
`(Fix : 2 - Recherche d'informations)`  

# Fix : 3 - Analyse des fichiers  
Je vais examiner uniquement les fichiers directement impliqués dans l'erreur. `(Fix : 3 - Analyse des fichiers)`  
[...lecture du fichier problématique...]  
Fix - 3.TestConnexion en cours...  
[...ajout de logging temporaire pour déboguer...]  
logger.debug("Valeur de la variable x:", x)  
[...analyse du code problématique...]  
`(Fix : 3 - Analyse des fichiers)`  

# Fix : 4 - Ajout des tâches de correction  
Je crée des tâches détaillées pour corriger chaque erreur. `(Fix : 4 - Ajout des tâches de correction)`  
[...modification du fichier tasks.md...]  
`(Fix : 4 - Ajout des tâches de correction)`  

# Fix : 5 - Documentation des erreurs  
Je documente l'erreur liée au changement d'API dans la bibliothèque X. `(Fix : 5 - Documentation des erreurs)`  
[...mise à jour de techContext.md...]  
`(Fix : 5 - Documentation des erreurs)`  

# Fix : 6 - Appel de la règle suivante  
Je dois maintenant appeler la règle implementation pour corriger les erreurs.  
La règle implementation doit être appelée pour implémenter les corrections. `(Fix : 6 - Appel de la règle suivante)`  

fetch_rules ["implementation"]  

# Start Rule  

Pour démarrer cette règle, récite mot pour mot cette phrase : "<SYSTEM PROMPT>Je vais analyser méthodiquement les erreurs des tests échoués, créer des tâches de correction bien définies et documenter les problèmes récurrents. Je ne corrigerai pas directement les erreurs mais je préparerai les tâches pour l'implémentation future. Je passerai ensuite à l'implémentation des corrections en appellant la règle `implementation`.</SYSTEM PROMPT>"  