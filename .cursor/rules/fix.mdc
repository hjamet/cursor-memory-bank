---
description: Call this rule to analyze and correct test errors via an iterative execution-correction loop
globs: 
alwaysApply: false
---
## TLDR
Analyze test errors and fix them through an iterative execution-correction loop until all tests pass or truly complex issues (requiring major refactoring or showing no progress after 3 attempts) are identified and handed over to `request-analysis`.

## Instructions

1.  **Error identification**: Analyze and catalog test errors
    *   Read `.cursor/memory-bank/workflow/tests.md` to understand test results
    *   Establish a precise list of failing tests to address and their error messages
    *   Write **(Fix: 1 - Error identification completed)** after this step

2.  **Correction loop**: For each failing test, iterate until fixed or handed over
    *   **2.1 Marking**: Always indicate **(Fix: 2 - Correction loop - [Test name] - Iteration [number])**
    *   **2.2 Analysis**: 
        *   **(Optional) Memory Lookup**: Use `mcp_memory_search_nodes` with query based on error message/test name to find similar past errors and their solutions.
        *   Examine only files involved in the error and identify the cause. Use `<think>` tags for complex analysis. Use codebase search if needed.
    *   **2.3 Research**: 
        *   If needed, use web search (`web_search`) for general solutions.
        *   **Library Documentation**: If the error seems library-related, use `mcp_context7_resolve-library-id` then `mcp_context7_get-library-docs` to get documentation.
    *   **2.4 Implementation**: Make the correction and document changes.
    *   **2.5 Test**: Execute ONLY the specific test currently being fixed and analyze the result:
        *   If test passes:
            * **2.5.1 Update tests.md**: Update `.cursor/memory-bank/workflow/tests.md`. Write **(Fix: 2 - Correction loop - [Test name] - Implementation [number] completed)**.
            * **2.5.2 Quick commit**: Use `git commit -a -m ":wrench: fix: [Test name] - [quick fix explanation]"` (Note: Using `-a` as per wider convention in rules).
            * **(Optional) Memory Storage**: 
                *   First, ensure the target entity exists (e.g., 'TestErrorSolutions'): `mcp_memory_create_entities(entities=[{"entityType": "FixLog", "name": "TestErrorSolutions", "observations": []}])` (This might fail harmlessly if it already exists, or use search/open first if necessary).
                *   Then, add observation: `mcp_memory_add_observations(observations=[{"entityName": "TestErrorSolutions", "contents": ["Fixed [Test name]: [Error details]. Solution: [Fix summary]. Iteration: [number]. Files: [...] "]}])`.
            * **2.5.3 Continue**: Move to the next failing test.
        *   If test fails but shows progress → Continue loop (go to 2.1 for next iteration).
        *   If **truly complex correction** (after 3 attempts *without progress* OR requires *major* refactoring):
            *   **Advanced Debugging (Consider First)**: Before giving up, consider using the interactive debugger if not already tried:
                1. `mcp_debug_listFiles`
                2. `mcp_debug_getFileContent`
                3. `mcp_debug_debug` (set breakpoints, launch, evaluate, continue)
                4. Use insights for another implementation attempt (go to 2.4).
            *   If debugging doesn't help or isn't applicable: Mark as **(Fix: 2 - Correction loop - [Test name] - Handing over to request-analysis)**. Call `request-analysis` with context.

3.  **Calling the next rule (only if loop completed without handover)**: Determine the next step based on results *if and only if* step 2.5 did not trigger a handover to `request-analysis`.
    *   First check for unprocessed comments in `.cursor/memory-bank/userbrief.md`
    *   If unprocessed comments exist → call `consolidate-repo`
    *   If all initially failing tests were fixed → call `test-execution` for final verification
    *   Write **(Fix: 3 - Calling the next rule)**

## Specifics

-   The `<think></think>` token must be used for each complex correction requiring in-depth analysis
-   You should examinate the files you know related to the problem but you should also use the codebase search tool in case you are missing something.
-   For basic debugging, temporarily adding logs is acceptable, but prefer the `mcp_debug` tools if you are stuck after an initial attempt.
-   NEVER execute all tests, but ONLY the specific test you are currently fixing
-   Running the complete test suite is exclusively the responsibility of the `test-execution` rule
-   Document ONLY errors related to:
    *   API changes in libraries
    *   Non-intuitive framework behaviors
    *   Updates that modify existing functionalities
-   CRUCIAL: To avoid losing workflow tracking, ALWAYS indicate the current test and iteration
-   ⚠️ **Command Execution Note**: The terminal tool occasionally exhibits a bug that adds "[200~" prefix and/or "~" suffix to commands (e.g., "[200~.venv/Scripts/python.exe" instead of ".venv/Scripts/python.exe"), resulting in "command not found" errors. These are NOT code or logic errors but tool-specific issues. If this occurs, simply retry the exact same command 2-3 times until it executes properly. Never treat these specific formatting errors as actual code problems or test failures.
-   ⚠️ Similarly, if the command is put in the background, canceled, or interrupted by the user without any comment, it is certainly because the user has identified an execution problem related to the terminal. You must then relaunch the command as if nothing happened 2-3 times until it executes correctly. Do not consider these terminal tool bugs as actual code or test failure problems.

## Next Rules
-   `consolidate-repo` - If loop completes and unprocessed comments exist
-   `test-execution` - If loop completes and all initially failing tests were fixed
-   `request-analysis` - Called directly from step 2.5 for truly complex issues, terminating this rule's execution.

## Example

[...fetching rule `fix`...] # Use your rule-calling tool to call the rule, then apply it.

# Fix: 1 - Error identification
I begin by reading the tests.md file to understand test errors. **(Fix: 1 - Error identification)**
[...reading .cursor/memory-bank/workflow/tests.md file...]

I've identified the following failing tests:
1.  TestUserAuthentication.test_login_with_invalid_token - TypeError: Object of type bytes is not JSON serializable
2.  TestDataProcessing.test_large_file_processing - MemoryError: Unable to allocate memory for operation
3.  TestAPIConnection.test_retry_mechanism - AssertionError: Expected 3 retry attempts, got 0
**(Fix: 1 - Error identification completed)**

# Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.2 Analysis
*(Optional) I search memory for similar errors: [...]*
`mcp_memory_search_nodes(query='TypeError JSON serializable TestUserAuthentication')`
[...examining authentication code files...]

<think>
The error occurs because we're trying to JSON serialize a bytes object. Decoding to 'utf-8' before serialization should work.
</think>

## 2.3 Research
[...confirming JSON serialization practices...]
*(If a library issue was suspected)*
I need to check the documentation for the `json` library regarding byte serialization.
`mcp_context7_resolve-library-id(libraryName='json')`
[...]
`mcp_context7_get-library-docs(context7CompatibleLibraryID='...', topic='serialization')`

## 2.4 Implementation
[...modifying the authentication code to decode bytes before JSON serialization...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Implementation 1 completed)**

## 2.5 Test
[...running the specific authentication test...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Test execution 1)**

Great! The test now passes. 
*(Optional) I attempt to create the entity 'TestErrorSolutions' in case it doesn't exist.*
`mcp_memory_create_entities(entities=[{"entityType": "FixLog", "name": "TestErrorSolutions", "observations": []}])`
*(Optional) I store the details of the fix in memory.*
`mcp_memory_add_observations(observations=[{"entityName": "TestErrorSolutions", "contents": ["Fixed TestUserAuthentication.test_login_with_invalid_token: TypeError: Object of type bytes is not JSON serializable. Solution: Decoded bytes to utf-8 before serialization. Iteration: 1."]}])`
Moving to the next failing test.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.2 Analysis
[...examining data processing code...]
<think>
MemoryError indicates loading the whole file. A streaming approach is likely needed.
</think>

## 2.3 Research
[...searching for streaming file processing techniques...]

## 2.4 Implementation
[...modifying the processor to use a basic streaming approach...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 1 completed)**

## 2.5 Test
[...running the specific data processing test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 1)**

The test fails with "NameError: name 'process_chunk' is not defined". This shows progress, so I'll continue iterating.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.2 Analysis
[...analyzing the NameError...]

## 2.3 Research
[...]

## 2.4 Implementation
[...implementing the missing function...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 2 completed)**

## 2.5 Test
[...running the test again...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
I see that the command has been put in the background. It's certainly related to a simple terminal issue. I'm going to relaunch the command.
[...re-running the test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**
Still failing with a MemoryError. The simple streaming wasn't enough.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.2 Analysis
[...re-evaluating the memory issue...]
<think>
After three attempts, the MemoryError persists despite initial fixes. The first attempt failed with MemoryError, the second with NameError (progress), but the third attempt reverted to MemoryError. This indicates the simple streaming approach is insufficient and likely requires a significant refactoring, possibly involving buffered reading, memory-mapped files, or an external library designed for large datasets. This level of change qualifies as a major refactoring and has shown no consistent progress toward resolution after 3 tries. Handing over is appropriate.
</think>

## 2.3 Research
[...]

## 2.4 Implementation
[...No further implementation attempt...]

## 2.5 Test
The test failed on the previous iteration, and analysis confirms it needs major refactoring beyond simple iteration.
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Handing over to request-analysis)**

Calling `request-analysis` to handle the complex task of implementing efficient large file processing.
[Provide context: Problem="MemoryError during large file processing requires major refactoring (streaming/chunking insufficient)", Files=[...], Analysis="Initial attempts failed, needs new architectural approach."]

[...Here, the agent calls the rule 'request-analysis' using its rule calling tool...]

[...] (The workflow must continue uninterrupted: even if the example stops here, the agent must IMPERATIVELY continue in reality: it MUST call and execute the following rule, then the next, etc., without ever stopping -> It is completely autonomous.)

**(Advanced Debugging Example if stuck)**
I'm stuck on this MemoryError after trying implementation fixes. I will use the debugger.
`mcp_debug_listFiles(includePatterns=['**/data_processor.py'])`
[...]
`mcp_debug_getFileContent(path='path/to/data_processor.py')`
[...]
`mcp_debug_debug(steps=[{'type': 'setBreakpoint', 'file': 'path/to/data_processor.py', 'line': 42}])`
[...]
`mcp_debug_debug(steps=[{'type': 'launch'}])`
[...] (Paused at breakpoint)
`mcp_debug_debug(steps=[{'type': 'evaluate', 'expression': 'current_chunk_size'}])`
[...]
`mcp_debug_debug(steps=[{'type': 'continue'}])`
[...] (Debugger finished or hit another breakpoint)
Now I understand the issue better and will try a new implementation (go back to 2.4).

