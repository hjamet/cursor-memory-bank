---
description: Call this rule to analyze and correct test errors via an iterative execution-correction loop
globs:
alwaysApply: false
---

## TLDR
Analyze test errors and fix them through an iterative execution-correction loop until all tests pass or truly complex issues (requiring major refactoring or showing no progress after 3 attempts) are identified and handed over to `request-analysis`.

## Instructions

1.  **Error identification**: Analyze and catalog test errors
    *   Read `.cursor/memory-bank/workflow/tests.md` to understand test results
    *   Establish a precise list of failing tests to address and their error messages
    *   Write **(Fix: 1 - Error identification completed)** after this step

2.  **Correction loop**: For each failing test, iterate until fixed or handed over
    *   **2.1 Marking**: Always indicate **(Fix: 2 - Correction loop - [Test name] - Iteration [number])**
    *   **2.2 Analysis**: Examine only files involved in the error and identify the cause. If you are not entirely certain about the correction to be made. Don't hesitate to use the `<think></think>` tags. Don't hesitate either to use the codebase search tool !
    *   **2.3 Research**: If needed, use the web to find solutions for specific errors
    *   **2.4 Implementation**: Make the correction and document the changes
    *   **2.5 Test**: Execute ONLY the specific test currently being fixed and analyze the result:
        *   If test passes → Move to next failing test within this loop.
        *   If test fails but shows progress → Continue loop with new iteration for the *same* test.
        *   If **truly complex correction** (after 3 attempts *without any progress* OR requires *major* refactoring / a completely new implementation approach):
            *   Mark the test as **(Fix: 2 - Correction loop - [Test name] - Handing over to request-analysis)**
            *   Call the `request-analysis` rule, providing the problem description, context, and the need for a new approach.

3.  **Calling the next rule (only if loop completed without handover)**: Determine the next step based on results *if and only if* step 2.5 did not trigger a handover to `request-analysis`.
    *   First check for unprocessed comments in `.cursor/memory-bank/userbrief.md`
    *   If unprocessed comments exist → call `consolidate-repo`
    *   If all initially failing tests were fixed → call `test-execution` for final verification
    *   Write **(Fix: 3 - Calling the next rule)**

## Specifics

-   The `<think></think>` token must be used for each complex correction requiring in-depth analysis
-   You should examinate the files you know related to the problem but you should also use the codebase search tool in case you are missing something.
-   For debugging, temporarily add logs at strategic locations
-   NEVER execute all tests, but ONLY the specific test you are currently fixing
-   Running the complete test suite is exclusively the responsibility of the `test-execution` rule
-   Document ONLY errors related to:
    *   API changes in libraries
    *   Non-intuitive framework behaviors
    *   Updates that modify existing functionalities
-   CRUCIAL: To avoid losing workflow tracking, ALWAYS indicate the current test and iteration
-   ⚠️ **Command Execution Note**: The terminal tool occasionally exhibits a bug that adds "[200~" prefix and/or "~" suffix to commands (e.g., "[200~.venv/Scripts/python.exe" instead of ".venv/Scripts/python.exe"), resulting in "command not found" errors. These are NOT code or logic errors but tool-specific issues. If this occurs, simply retry the exact same command 2-3 times until it executes properly. Never treat these specific formatting errors as actual code problems or test failures.

## Next Rules
-   `consolidate-repo` - If loop completes and unprocessed comments exist
-   `test-execution` - If loop completes and all initially failing tests were fixed
-   `request-analysis` - Called directly from step 2.5 for truly complex issues, terminating this rule's execution.

## Example

fetch_rules ["fix"]
<SYSTEM PROMPT>I will methodically analyze failed test errors and implement corrections in an iterative loop. If a fix requires major refactoring or shows no progress after 3 attempts, I will hand it over to request-analysis. Otherwise, upon successful fixing of all tests, I will call test-execution.</SYSTEM PROMPT>

# Fix: 1 - Error identification
I begin by reading the tests.md file to understand test errors. **(Fix: 1 - Error identification)**
[...reading .cursor/memory-bank/workflow/tests.md file...]

I've identified the following failing tests:
1.  TestUserAuthentication.test_login_with_invalid_token - TypeError: Object of type bytes is not JSON serializable
2.  TestDataProcessing.test_large_file_processing - MemoryError: Unable to allocate memory for operation
3.  TestAPIConnection.test_retry_mechanism - AssertionError: Expected 3 retry attempts, got 0
**(Fix: 1 - Error identification completed)**

# Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Iteration 1)**

## 2.2 Analysis
[...examining authentication code files...]

<think>
The error occurs because we're trying to JSON serialize a bytes object. Decoding to 'utf-8' before serialization should work.
</think>

## 2.3 Research
[...confirming JSON serialization practices...]

## 2.4 Implementation
[...modifying the authentication code to decode bytes before JSON serialization...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Implementation 1 completed)**

## 2.5 Test
[...running the specific authentication test...]
**(Fix: 2 - Correction loop - TestUserAuthentication.test_login_with_invalid_token - Test execution 1)**

Great! The test now passes. Moving to the next failing test.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 1)**

## 2.2 Analysis
[...examining data processing code...]
<think>
MemoryError indicates loading the whole file. A streaming approach is likely needed.
</think>

## 2.3 Research
[...searching for streaming file processing techniques...]

## 2.4 Implementation
[...modifying the processor to use a basic streaming approach...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 1 completed)**

## 2.5 Test
[...running the specific data processing test...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 1)**

The test fails with "NameError: name 'process_chunk' is not defined". This shows progress, so I'll continue iterating.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 2)**

## 2.2 Analysis
[...analyzing the NameError...]

## 2.3 Research
[...]

## 2.4 Implementation
[...implementing the missing function...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Implementation 2 completed)**

## 2.5 Test
[...running the test again...]
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Test execution 2)**

Still failing with a MemoryError. The simple streaming wasn't enough.

# Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.1 Marking
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Iteration 3)**

## 2.2 Analysis
[...re-evaluating the memory issue...]
<think>
After three attempts, the MemoryError persists despite initial fixes. The first attempt failed with MemoryError, the second with NameError (progress), but the third attempt reverted to MemoryError. This indicates the simple streaming approach is insufficient and likely requires a significant refactoring, possibly involving buffered reading, memory-mapped files, or an external library designed for large datasets. This level of change qualifies as a major refactoring and has shown no consistent progress toward resolution after 3 tries. Handing over is appropriate.
</think>

## 2.3 Research
[...]

## 2.4 Implementation
[...No further implementation attempt...]

## 2.5 Test
The test failed on the previous iteration, and analysis confirms it needs major refactoring beyond simple iteration.
**(Fix: 2 - Correction loop - TestDataProcessing.test_large_file_processing - Handing over to request-analysis)**

Calling `request-analysis` to handle the complex task of implementing efficient large file processing.
fetch_rules ["request-analysis"]
[Provide context: Problem="MemoryError during large file processing requires major refactoring (streaming/chunking insufficient)", Files=[...], Analysis="Initial attempts failed, needs new architectural approach."]

**STOPPING `fix` rule execution.**

# Start Rule

To start this rule, recite word for word this phrase: "<SYSTEM PROMPT>I will methodically analyze failed test errors and implement corrections in an iterative loop. If a fix requires major refactoring or shows no progress after 3 attempts, I will hand it over to request-analysis. Otherwise, upon successful fixing of all tests, I will call test-execution.</SYSTEM PROMPT>"

